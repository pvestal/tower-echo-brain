#!/usr/bin/env python3
"""
Proper Anime Video Generation Service
Port: 8328
Generates KB-quality videos using ComfyUI with AOM3A1B.safetensors
"""

from fastapi import FastAPI
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from fastapi.staticfiles import StaticFiles
from fastapi.responses import FileResponse
from fastapi import HTTPException
from fastapi.background import BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, List, Optional, Any
import uvicorn
import json
import requests
import uuid
import os
import time
import shutil
from pathlib import Path
from datetime import datetime
import subprocess
import logging
from logging.handlers import RotatingFileHandler
import asyncio
from quality_integration import assess_video_quality, QUALITY_ENABLED
from error_handler import ErrorHandler, ComfyUIConnectionError, VRAMShortageError, GenerationTimeoutError, ErrorCategory, ErrorSeverity

# PHASE 1 FIX: Structured logging with rotation
log_dir = Path("/opt/tower-anime-production/logs")
log_dir.mkdir(exist_ok=True, parents=True)

# Configure root logger
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        RotatingFileHandler(
            log_dir / "anime_service.log",
            maxBytes=10_000_000,  # 10MB
            backupCount=5
        ),
        logging.StreamHandler()  # Also log to console
    ]
)
logger = logging.getLogger(__name__)
logger.info("="*60)
logger.info("ANIME SERVICE STARTING - Phase 2 Production Hardening")
logger.info("="*60)
# Initialize error handler
error_handler = ErrorHandler(log_dir)
logger.info("‚úÖ Error handler initialized")

# PHASE 1 FIX: VRAM monitoring with pynvml
try:
    import pynvml
    pynvml.nvmlInit()
    gpu_handle = pynvml.nvmlDeviceGetHandleByIndex(0)
    GPU_MONITORING_ENABLED = True
    logger.info("‚úÖ GPU monitoring enabled (pynvml)")
except Exception as e:
    GPU_MONITORING_ENABLED = False
    logger.warning(f"‚ö†Ô∏è  GPU monitoring disabled: {e}")

# Add Echo Brain quality integration
import sys
sys.path.append("/opt/tower-echo-brain/src/services")
sys.path.append("/opt/tower-echo-brain/routing")
try:
    from quality_assessment import VideoQualityAssessment
    from feedback_system import FeedbackProcessor, FeedbackType
    from echo_integration import EchoIntegration
    echo = EchoIntegration()
    quality_assessor = VideoQualityAssessment()
    feedback_processor = FeedbackProcessor(db_config={"host": "localhost", "database": "anime_production"})
    QUALITY_ENABLED = True
    logger.info("‚úÖ Quality systems enabled")
except ImportError as e:
    logger.warning(f"‚ö†Ô∏è  Quality systems not available: {e}")
    QUALITY_ENABLED = False
app = FastAPI(title="Tower Anime Video Service", version="2.0.0-phase2")

# Enable CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# PHASE 2C: Scheduled cleanup task
@app.on_event("startup")
async def startup_event():
    """Start background tasks on service startup"""
    import asyncio

    async def periodic_cleanup():
        """Run cleanup every hour"""
        while True:
            await asyncio.sleep(3600)  # Wait 1 hour
            logger.info("üßπ Starting scheduled cleanup...")
            await cleanup_orphaned_files()

    # Start cleanup task in background
    asyncio.create_task(periodic_cleanup())
    logger.info("‚úÖ Scheduled cleanup task started (runs every hour)")

# Configuration
COMFYUI_URL = "http://127.0.0.1:8188"
OUTPUT_DIR = Path("/mnt/1TB-storage/ComfyUI/output")
OUTPUT_DIR.mkdir(exist_ok=True, parents=True)

# PHASE 1 FIX: Production limits
MAX_CONCURRENT_GENERATIONS = 3  # Safe for 12GB VRAM (4GB per generation)
VRAM_THRESHOLD_GB = 4.0  # Require 4GB free before accepting generation
MAX_STATUS_HISTORY = 100  # Prevent memory leak in status tracker
CLEANUP_AGE_HOURS = 24  # Clean up failed generations after 24 hours

# PHASE 2B: Retry configuration
MAX_RETRIES = 3
INITIAL_RETRY_DELAY = 2  # seconds
MAX_RETRY_DELAY = 30  # seconds

# PHASE 2D: Apple Music rate limiting
APPLE_MUSIC_MAX_REQUESTS = 60  # per minute
APPLE_MUSIC_WINDOW_SECONDS = 60

# PHASE 1 FIX: Concurrency control
generation_semaphore = asyncio.Semaphore(MAX_CONCURRENT_GENERATIONS)
logger.info(f"‚úÖ Concurrency limit: {MAX_CONCURRENT_GENERATIONS} simultaneous generations")
logger.info(f"‚úÖ VRAM threshold: {VRAM_THRESHOLD_GB}GB required free")
logger.info(f"‚úÖ Retry configuration: {MAX_RETRIES} retries with exponential backoff")
logger.info(f"‚úÖ Rate limiting: {APPLE_MUSIC_MAX_REQUESTS} Apple Music requests per {APPLE_MUSIC_WINDOW_SECONDS}s")

# Data models
class AnimeGenerationRequest(BaseModel):
    prompt: str
    character: str = "magical anime character"
    duration: int = 5
    frames: int = 120  # 24fps * 5 seconds (FIXED)
    style: str = "anime masterpiece"
    width: int = 768  # Production quality (VRAM-optimized)
    height: int = 768  # Production quality (VRAM-optimized)
    fps: int = 24
    use_apple_music: bool = False
    track_id: Optional[str] = None

class VideoGenerationStatus:
    """PHASE 1 FIX: Thread-safe status tracking with memory leak prevention"""
    def __init__(self):
        self.generations = {}
        self.lock = asyncio.Lock()
        self.max_history = MAX_STATUS_HISTORY
        logger.info(f"‚úÖ Status tracker initialized (max history: {self.max_history})")

    async def set_status(self, gen_id: str, status: str, progress: int = 0, message: str = "", output_file: str = ""):
        """Thread-safe status update with automatic cleanup"""
        async with self.lock:
            # Check if we need to cleanup old entries
            if len(self.generations) >= self.max_history:
                await self._cleanup_old_entries()

            self.generations[gen_id] = {
                "status": status,
                "progress": progress,
                "message": message,
                "output_file": output_file,
                "timestamp": datetime.now().isoformat(),
                "updated_at": time.time()
            }
            logger.debug(f"Status updated [{gen_id[:8]}]: {status} ({progress}%) - {message}")

    async def get_status(self, gen_id: str):
        """Thread-safe status retrieval"""
        async with self.lock:
            return self.generations.get(gen_id, {"status": "not_found"})

    async def _cleanup_old_entries(self):
        """Remove oldest completed/failed entries to prevent memory leak"""
        completed_failed = {
            k: v for k, v in self.generations.items()
            if v['status'] in ['completed', 'failed']
        }

        if completed_failed:
            # Remove oldest entry
            oldest_key = min(
                completed_failed.keys(),
                key=lambda k: completed_failed[k].get('updated_at', 0)
            )
            del self.generations[oldest_key]
            logger.info(f"Cleaned up old status entry: {oldest_key[:8]} ({self.generations[oldest_key]['status']})")

    async def get_active_count(self) -> int:
        """Get count of actively processing generations"""
        async with self.lock:
            return len([v for v in self.generations.values() if v['status'] == 'generating'])

status_tracker = VideoGenerationStatus()

# PHASE 2D: Apple Music Rate Limiter
class RateLimiter:
    """Simple rate limiter using sliding window"""
    def __init__(self, max_requests: int, window_seconds: int):
        self.max_requests = max_requests
        self.window_seconds = window_seconds
        self.requests = []
        self.lock = asyncio.Lock()
        logger.info(f"‚úÖ Rate limiter initialized: {max_requests} requests per {window_seconds}s")

    async def acquire(self):
        """Check if request is allowed under rate limit"""
        async with self.lock:
            now = time.time()
            # Remove requests outside the window
            self.requests = [r for r in self.requests if now - r < self.window_seconds]

            if len(self.requests) >= self.max_requests:
                oldest_request = min(self.requests)
                wait_time = self.window_seconds - (now - oldest_request)
                logger.warning(f"‚ö†Ô∏è  Rate limit reached, need to wait {wait_time:.1f}s")
                raise HTTPException(
                    status_code=429,
                    detail={
                        "error": "Rate limit exceeded",
                        "retry_after": int(wait_time) + 1,
                        "message": f"Too many requests. Please wait {int(wait_time) + 1} seconds."
                    }
                )

            self.requests.append(now)
            logger.debug(f"Rate limiter: {len(self.requests)}/{self.max_requests} requests in window")

apple_music_limiter = RateLimiter(APPLE_MUSIC_MAX_REQUESTS, APPLE_MUSIC_WINDOW_SECONDS)

# PHASE 2B: Retry decorator with exponential backoff
async def retry_with_backoff(func, *args, max_retries=MAX_RETRIES, **kwargs):
    """Retry a function with exponential backoff"""
    last_exception = None

    for attempt in range(max_retries):
        try:
            return await func(*args, **kwargs) if asyncio.iscoroutinefunction(func) else func(*args, **kwargs)
        except Exception as e:
            last_exception = e
            if attempt < max_retries - 1:
                delay = min(INITIAL_RETRY_DELAY * (2 ** attempt), MAX_RETRY_DELAY)
                logger.warning(f"‚ö†Ô∏è  Retry {attempt + 1}/{max_retries} after {delay}s: {str(e)[:100]}")
                await asyncio.sleep(delay)
            else:
                logger.error(f"‚ùå All {max_retries} retries failed: {str(e)}")

    raise last_exception

# PHASE 2C: File cleanup system
async def cleanup_orphaned_files():
    """Clean up orphaned video files older than CLEANUP_AGE_HOURS"""
    try:
        # OUTPUT_DIR same as OUTPUT_DIR now
        # Only check OUTPUT_DIR now
        cutoff_time = time.time() - (CLEANUP_AGE_HOURS * 3600)

        cleaned_count = 0
        total_size = 0

        # Check both output directories
        for directory in [OUTPUT_DIR]:
            if not directory.exists():
                continue

            for video_file in directory.glob("*.mp4"):
                try:
                    if video_file.stat().st_mtime < cutoff_time:
                        # Check if file is referenced in active generations
                        file_path = str(video_file)
                        is_active = False

                        async with status_tracker.lock:
                            for gen_id, status in status_tracker.generations.items():
                                if status.get('output_file') == file_path:
                                    if status['status'] in ['generating', 'initializing', 'submitting']:
                                        is_active = True
                                        break

                        if not is_active:
                            file_size = video_file.stat().st_size
                            video_file.unlink()
                            cleaned_count += 1
                            total_size += file_size
                            logger.info(f"üóëÔ∏è  Cleaned orphaned file: {video_file.name} ({file_size / 1024:.1f}KB)")

                except Exception as e:
                    logger.warning(f"Error cleaning {video_file}: {e}")

        if cleaned_count > 0:
            logger.info(f"‚úÖ Cleanup complete: {cleaned_count} files removed ({total_size / 1024 / 1024:.1f}MB freed)")
        else:
            logger.debug("No orphaned files found for cleanup")

    except Exception as e:
        logger.error(f"Error during cleanup: {e}")

async def check_vram_available() -> tuple[bool, float, str]:
    """PHASE 1 FIX: Check if sufficient VRAM is available"""
    if not GPU_MONITORING_ENABLED:
        return True, 0.0, "GPU monitoring disabled"

    try:
        mem_info = pynvml.nvmlDeviceGetMemoryInfo(gpu_handle)
        free_gb = mem_info.free / (1024**3)
        used_gb = mem_info.used / (1024**3)
        total_gb = mem_info.total / (1024**3)

        is_available = free_gb >= VRAM_THRESHOLD_GB
        status_msg = f"{free_gb:.2f}GB free ({used_gb:.2f}GB used / {total_gb:.2f}GB total)"

        if not is_available:
            logger.warning(f"‚ö†Ô∏è  Insufficient VRAM: {status_msg}")

        return is_available, free_gb, status_msg
    except Exception as e:
        logger.error(f"Error checking VRAM: {e}")
        return True, 0.0, f"Error: {str(e)}"

@app.get("/api/health")
async def health_check():
    """Check service health and dependencies with detailed metrics"""
    comfyui_status = "disconnected"
    comfyui_details = {}

    try:
        response = requests.get(f"{COMFYUI_URL}/system_stats", timeout=2)
        if response.status_code == 200:
            comfyui_status = "connected"
            comfyui_details = response.json()
    except Exception as e:
        logger.debug(f"ComfyUI health check failed: {e}")

    # Get VRAM status
    vram_available, vram_free, vram_status = await check_vram_available()

    # Get active generation count
    active_count = await status_tracker.get_active_count()

    health = {
        "status": "healthy" if comfyui_status == "connected" and vram_available else "degraded",
        "service": "Tower Anime Video Service",
        "version": "2.0.0-phase2",
        "phase": "Phase 2 Production Hardening (Retry + Rate Limiting + Cleanup)",
        "comfyui_status": comfyui_status,
        "output_dir": str(OUTPUT_DIR),
        "model": "AOM3A1B.safetensors",
        "timestamp": datetime.now().isoformat(),
        "capacity": {
            "max_concurrent": MAX_CONCURRENT_GENERATIONS,
            "active_generations": active_count,
            "available_slots": MAX_CONCURRENT_GENERATIONS - active_count
        },
        "vram": {
            "monitoring_enabled": GPU_MONITORING_ENABLED,
            "available": vram_available,
            "free_gb": round(vram_free, 2),
            "threshold_gb": VRAM_THRESHOLD_GB,
            "status": vram_status
        },
        "features": {
            "thread_safe_status": True,
            "concurrency_limits": True,
            "vram_monitoring": GPU_MONITORING_ENABLED,
            "structured_logging": True,
            "apple_music": True,
            "quality_assessment": QUALITY_ENABLED,
            "retry_logic": True,
            "exponential_backoff": True,
            "rate_limiting": True,
            "auto_cleanup": True,
            "cleanup_age_hours": CLEANUP_AGE_HOURS
        },
        "phase_2_config": {
            "max_retries": MAX_RETRIES,
            "initial_retry_delay": INITIAL_RETRY_DELAY,
            "max_retry_delay": MAX_RETRY_DELAY,
            "apple_music_rate_limit": f"{APPLE_MUSIC_MAX_REQUESTS} requests per {APPLE_MUSIC_WINDOW_SECONDS}s"
        }
    }

    return health

@app.post("/generate/professional")
async def generate_professional_video(request: AnimeGenerationRequest, background_tasks: BackgroundTasks):
    """Generate professional quality anime video with production hardening"""
    generation_id = str(uuid.uuid4())

    logger.info(f"üé¨ Generation request received: {generation_id[:8]} - prompt: '{request.prompt[:50]}...'")

    # PHASE 1 FIX: Check VRAM before accepting request
    vram_available, vram_free, vram_status = await check_vram_available()
    if not vram_available:
        logger.error(f"‚ùå Generation rejected [{generation_id[:8]}]: Insufficient VRAM - {vram_status}")
        raise HTTPException(
            status_code=503,
            detail={
                "error": "Insufficient GPU memory",
                "vram_status": vram_status,
                "retry_after": 30,
                "message": "GPU is currently processing other requests. Please try again in a moment."
            }
        )

    # PHASE 1 FIX: Check concurrency limit
    active_count = await status_tracker.get_active_count()
    if active_count >= MAX_CONCURRENT_GENERATIONS:
        logger.warning(f"‚ö†Ô∏è  Generation queued [{generation_id[:8]}]: At concurrency limit ({active_count}/{MAX_CONCURRENT_GENERATIONS})")

    # Initialize status
    await status_tracker.set_status(
        generation_id,
        "initializing",
        0,
        "Preparing video generation workflow..."
    )

    # Start generation in background with semaphore control
    background_tasks.add_task(
        generate_video_with_semaphore,
        generation_id,
        request
    )

    logger.info(f"‚úÖ Generation accepted [{generation_id[:8]}]: VRAM={vram_free:.2f}GB, Active={active_count}/{MAX_CONCURRENT_GENERATIONS}")

    return {
        "generation_id": generation_id,
        "status": "started",
        "message": "Professional video generation started",
        "estimated_time": "2-5 minutes for 4K video",
        "check_status_url": f"/api/status/{generation_id}",
        "vram_available_gb": round(vram_free, 2),
        "queue_position": active_count + 1
    }

@app.get("/api/status/{generation_id}")
async def get_generation_status(generation_id: str):
    """Get generation status"""
    status = await status_tracker.get_status(generation_id)
    if status["status"] == "not_found":
        raise HTTPException(status_code=404, detail="Generation not found")
    return status

async def generate_video_with_semaphore(generation_id: str, request: AnimeGenerationRequest):
    """PHASE 1 FIX: Wrapper with semaphore for concurrency control"""
    async with generation_semaphore:
        logger.info(f"üîí Acquired generation slot [{generation_id[:8]}]")
        try:
            await generate_video_async(generation_id, request)
        finally:
            logger.info(f"üîì Released generation slot [{generation_id[:8]}]")

async def generate_video_async(generation_id: str, request: AnimeGenerationRequest):
    """Generate video asynchronously with proper ComfyUI workflow"""
    start_time = time.time()
    try:
        # Update status
        await status_tracker.set_status(
            generation_id,
            "creating_workflow",
            10,
            "Creating 4K video workflow..."
        )

        # Create the workflow using our proven working template
        workflow = create_svd_video_workflow(request, generation_id)

        logger.info(f"üìã Created workflow for generation {generation_id[:8]}")

        # Submit to ComfyUI with retry logic (PHASE 2B)
        await status_tracker.set_status(
            generation_id,
            "submitting",
            25,
            "Submitting to ComfyUI..."
        )

        # Wrap ComfyUI submission with retry logic
        # DEBUG: Log workflow payload
        #         logger.info(f"üîç DEBUG - Node 5 batch_size: {workflow['5']['inputs']['batch_size']}")
        #         logger.info(f"üîç DEBUG - Node 9 context_length: {workflow['9']['inputs']['context_length']}")
        #         logger.info(f"üîç DEBUG - Node 8 frame_rate: {workflow['8']['inputs']['frame_rate']}")
        import json
        logger.info(f"üîç DEBUG - Full workflow: {json.dumps(workflow, indent=2)}")

        def submit_to_comfyui():
            response = requests.post(
                f"{COMFYUI_URL}/prompt",
                json={"prompt": workflow},
                timeout=30
            )
            if response.status_code != 200:
                raise Exception(f"ComfyUI submission failed: {response.text}")
            return response.json()

        result = await retry_with_backoff(submit_to_comfyui)
        prompt_id = result.get('prompt_id')

        if not prompt_id:
            raise Exception("No prompt_id returned from ComfyUI")

        logger.info(f"üì§ Submitted to ComfyUI [{generation_id[:8]}] prompt_id: {prompt_id}")

        # Monitor progress
        await status_tracker.set_status(
            generation_id,
            "generating",
            50,
            f"Generating 4K video... (ComfyUI ID: {prompt_id})"
        )

        # Wait for completion with timeout
        max_wait = max(1800, request.frames * 5)  # 15 minutes for long videos
        wait_time = 0

        while wait_time < max_wait:
            await asyncio.sleep(5)
            wait_time += 5

            # Update progress
            progress = min(50 + (wait_time / max_wait) * 40, 90)
            await status_tracker.set_status(
                generation_id,
                "generating",
                int(progress),
                f"Processing frame generation... ({wait_time}s elapsed)"
            )

            # PHASE 2B: Check ComfyUI history with crash recovery
            try:
                def check_history():
                    history = requests.get(f"{COMFYUI_URL}/history/{prompt_id}", timeout=10)
                    if history.status_code != 200:
                        raise Exception(f"ComfyUI history check failed: HTTP {history.status_code}")
                    return history.json()

                # Retry history check if ComfyUI is temporarily unresponsive
                data = await retry_with_backoff(check_history, max_retries=2)

                if prompt_id in data:
                    prompt_status = data[prompt_id].get('status', {})
                    if prompt_status.get('status_str') == 'success' and prompt_status.get('completed'):
                        # Generation complete - look for output
                        outputs = data[prompt_id].get('outputs', {})
                        logger.info(f"‚úÖ ComfyUI generation completed [{generation_id[:8]}]")

                        # Find the video file
                        video_file = await find_generated_video(generation_id)
                        if video_file:
                            # Add Apple Music if requested
                            if request.use_apple_music and request.track_id:
                                await status_tracker.set_status(
                                    generation_id,
                                    "adding_music",
                                    95,
                                    "Adding Apple Music preview..."
                                )
                                video_file = await merge_apple_music_audio(video_file, request.track_id, generation_id)

                            elapsed = time.time() - start_time
                            await status_tracker.set_status(
                                generation_id,
                                "completed",
                                100,
                                f"4K video generation completed in {elapsed:.1f}s!",
                                video_file
                            )
                            logger.info(f"üéâ Video generation completed [{generation_id[:8]}]: {video_file} ({elapsed:.1f}s)")
                            
                            # PHASE 2: Quality Assessment Integration
                            if QUALITY_ENABLED:
                                try:
                                    quality_result = await assess_video_quality(video_file, generation_id)
                                    logger.info(f"üìä Quality assessment result: {quality_result.get("overall_score", "N/A")}")
                                except Exception as e:
                                    logger.warning(f"Quality assessment failed (non-blocking): {e}")

                            return
                    elif prompt_status.get('status_str') == 'error':
                        error_msg = prompt_status.get('messages', 'Unknown ComfyUI error')
                        raise Exception(f"ComfyUI generation failed: {error_msg}")
            except Exception as e:
                # Log warning but continue polling (ComfyUI might recover)
                logger.warning(f"‚ö†Ô∏è  ComfyUI status check error (will retry): {str(e)[:100]}")

        # Timeout
        raise Exception(f"Video generation timeout after {max_wait} seconds")

    except Exception as e:
        elapsed = time.time() - start_time
        logger.error(f"‚ùå Video generation failed [{generation_id[:8]}] after {elapsed:.1f}s: {str(e)}", exc_info=True)
        await status_tracker.set_status(
            generation_id,
            "failed",
            0,
            f"Generation failed: {str(e)}"
        )

async def find_generated_video(generation_id: str) -> str:
    """Find the generated video file and move it to proper location"""
    try:
        # ComfyUI saves videos to output directory with our prefix
        # OUTPUT_DIR same as OUTPUT_DIR now

        # Look for recent video files with our generation ID
        import glob
        video_patterns = [
            f"anime_video_{generation_id}*.mp4",
            f"anime_video_*.mp4"  # Fallback to any recent anime video
        ]

        found_files = []
        for pattern in video_patterns:
            found_files.extend(glob.glob(str(OUTPUT_DIR / pattern)))

        if not found_files:
            # Check for any recent video files (last 2 minutes)
            # time module already imported globally
            cutoff_time = time.time() - 120
            for file_path in OUTPUT_DIR.glob("*.mp4"):
                if file_path.stat().st_mtime > cutoff_time:
                    found_files.append(str(file_path))

        if found_files:
            # Use the most recent file (already in OUTPUT_DIR)
            latest_file = max(found_files, key=os.path.getctime)
            logger.info(f"Found video: {latest_file}")
            return str(latest_file)

        return None

    except Exception as e:
        logger.error(f"Error finding generated video: {e}")
        return None


async def merge_apple_music_audio(video_file: str, track_id: str, generation_id: str) -> str:
    """Download Apple Music preview and merge with video using ffmpeg"""
    try:
        # PHASE 2D: Apply rate limiting before Apple Music request
        await apple_music_limiter.acquire()

        logger.info(f"Downloading Apple Music preview for track {track_id}")

        # PHASE 2B: Download preview with retry logic
        def download_preview():
            response = requests.post(
                f'http://127.0.0.1:8315/api/apple-music/track/{track_id}/download',
                timeout=30
            )
            if response.status_code != 200:
                raise Exception(f"Apple Music API error: {response.text}")
            return response.json()

        data = await retry_with_backoff(download_preview)

        if not data:
            logger.error(f"Failed to download Apple Music preview")
            return video_file

        audio_file = data.get('file_path')
        
        if not audio_file or not os.path.exists(audio_file):
            logger.error(f"Audio file not found: {audio_file}")
            return video_file
        
        logger.info(f"Downloaded audio to: {audio_file}")
        
        # Create output path for video with music
        video_path = Path(video_file)
        output_file = video_path.parent / f"{video_path.stem}_with_music{video_path.suffix}"
        
        # Use ffmpeg to merge video and audio (30-sec preview loops to match video duration)
        ffmpeg_cmd = [
            'ffmpeg', '-y',
            '-i', video_file,
            '-stream_loop', '-1',  # Loop audio to match video duration
            '-i', audio_file,
            '-c:v', 'copy',  # Copy video codec (no re-encoding)
            '-c:a', 'aac',   # AAC audio codec
            '-shortest',     # Match shortest stream (video)
            '-map', '0:v:0', # Map video from first input
            '-map', '1:a:0', # Map audio from second input
            str(output_file)
        ]
        
        logger.info(f"Running ffmpeg: {' '.join(ffmpeg_cmd)}")
        result = subprocess.run(ffmpeg_cmd, capture_output=True, text=True)
        
        if result.returncode != 0:
            logger.error(f"ffmpeg failed: {result.stderr}")
            return video_file
        
        logger.info(f"Successfully merged audio, output: {output_file}")
        return str(output_file)
        
    except Exception as e:
        logger.error(f"Error merging Apple Music audio: {e}")
        return video_file


def create_svd_video_workflow(request: AnimeGenerationRequest, generation_id: str) -> Dict[str, Any]:
    """Create Stable Video Diffusion workflow for high-quality video generation
    
    Two-stage process:
    1. Generate high-quality init image from text prompt (anime model)
    2. Use SVD to create smooth video motion from init image
    """
    
    # Enhanced prompt for anime quality
    enhanced_prompt = f"anime masterpiece, {request.prompt}, {request.character}, studio quality, highly detailed, vibrant colors, professional illustration"
    
    # SVD optimal settings
    width = 1024  # SVD native resolution
    height = 576  # SVD native aspect ratio (16:9)
    video_frames = min(request.frames, 25)  # SVD works best with 14-25 frames
    fps = request.fps if request.fps <= 30 else 30  # SVD optimal fps range
    
    workflow = {
        # ===== STAGE 1: Generate Init Image =====
        
        # 1. Load anime checkpoint for init image
        "1": {
            "class_type": "CheckpointLoaderSimple",
            "inputs": {
                "ckpt_name": "AOM3A1B.safetensors"
            }
        },
        
        # 2. Text encode positive (init image)
        "2": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": enhanced_prompt,
                "clip": ["1", 1]
            }
        },
        
        # 3. Text encode negative (init image)
        "3": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": "low quality, blurry, bad anatomy, deformed, ugly, distorted, multiple subjects",
                "clip": ["1", 1]
            }
        },
        
        # 4. Empty latent for init image (single frame)
        "4": {
            "class_type": "EmptyLatentImage",
            "inputs": {
                "width": width,
                "height": height,
                "batch_size": 1  # Single init image
            }
        },
        
        # 5. KSampler for init image generation
        "5": {
            "class_type": "KSampler",
            "inputs": {
                "seed": int(time.time()),
                "steps": 35,  # High quality init image
                "cfg": 8.0,  # Strong prompt adherence
                "sampler_name": "dpmpp_2m",
                "scheduler": "karras",
                "positive": ["2", 0],
                "negative": ["3", 0],
                "latent_image": ["4", 0],
                "model": ["1", 0],
                "denoise": 1.0
            }
        },
        
        # 6. VAE Decode init image
        "6": {
            "class_type": "VAEDecode",
            "inputs": {
                "samples": ["5", 0],
                "vae": ["1", 2]
            }
        },
        
        # ===== STAGE 2: SVD Image-to-Video =====
        
        # 7. Load SVD model
        "7": {
            "class_type": "ImageOnlyCheckpointLoader",
            "inputs": {
                "ckpt_name": "svd_xt.safetensors"
            }
        },
        
        # 8. SVD Conditioning (convert image to video conditioning)
        "8": {
            "class_type": "SVD_img2vid_Conditioning",
            "inputs": {
                "clip_vision": ["7", 1],
                "init_image": ["6", 0],  # From Stage 1
                "vae": ["7", 2],
                "width": width,
                "height": height,
                "video_frames": video_frames,
                "motion_bucket_id": 127,  # Motion intensity (1-1023, 127 is balanced)
                "fps": fps,
                "augmentation_level": 0.0  # No augmentation for clean anime
            }
        },
        
        # 9. Video Linear CFG Guidance (required for SVD)
        "9": {
            "class_type": "VideoLinearCFGGuidance",
            "inputs": {
                "model": ["7", 0],
                "min_cfg": 1.0  # Minimum CFG for video frames
            }
        },
        
        # 10. KSampler for video generation
        "10": {
            "class_type": "KSampler",
            "inputs": {
                "seed": int(time.time()) + 1,  # Different seed than init
                "steps": 25,  # SVD optimal steps
                "cfg": 2.5,  # SVD uses lower CFG (2.0-3.5 range)
                "sampler_name": "euler",  # SVD works best with euler
                "scheduler": "karras",
                "positive": ["8", 0],  # SVD positive conditioning
                "negative": ["8", 1],  # SVD negative conditioning
                "latent_image": ["8", 2],  # SVD latent
                "model": ["9", 0],  # Patched model with CFG guidance
                "denoise": 1.0
            }
        },
        
        # 11. VAE Decode video frames
        "11": {
            "class_type": "VAEDecode",
            "inputs": {
                "samples": ["10", 0],
                "vae": ["7", 2]
            }
        },
        
        # 12. Combine frames into video
        "12": {
            "class_type": "VHS_VideoCombine",
            "inputs": {
                "images": ["11", 0],
                "frame_rate": fps,
                "loop_count": 0,
                "filename_prefix": f"anime_svd_{generation_id}",
                "format": "video/h264-mp4",
                "pingpong": False,
                "save_output": True
            }
        }
    }
    
    return workflow
def create_4k_video_workflow(request: AnimeGenerationRequest, generation_id: str) -> Dict[str, Any]:
    """Create PROPER AnimateDiff video workflow - NO MORE SLIDESHOWS!"""

    # Enhanced prompt for anime quality
    enhanced_prompt = f"anime masterpiece, {request.prompt}, {request.character}, studio quality, detailed animation, smooth motion, vibrant colors"

    # Conservative settings for RTX 3060 12GB VRAM
    # Updated settings for better quality (still safe for RTX 3060 12GB VRAM)
    width = min(request.width, 1024)  # Allow up to 1024x1024
    height = min(request.height, 1024)
    frames = request.frames  # NO CAP! Context window handles it
    workflow = {
        # 1. Load the anime checkpoint
        "1": {
            "class_type": "CheckpointLoaderSimple",
            "inputs": {
                "ckpt_name": "AOM3A1B.safetensors"
            }
        },

        # 2. **CRITICAL**: Load AnimateDiff motion model (THIS WAS MISSING!)
        "2": {
            "class_type": "ADE_AnimateDiffLoaderGen1",
            "inputs": {
                "model_name": "mm-Stabilized_high.pth",
                "beta_schedule": "sqrt_linear (AnimateDiff)",
                "model": ["1", 0],
                "context_options": ["9", 0],
            }
        },

        # 9. CONTEXT WINDOW FOR UNLIMITED FRAMES
        "9": {
            "class_type": "ADE_LoopedUniformContextOptions",
            "inputs": {
                "context_length": 24,
                "context_stride": 1,
                "context_overlap": 8,  # Smoother transitions
                "fuse_method": "pyramid",
                "closed_loop": True,
                "use_on_equal_length": False
            }
        },


        # 3. Text encode positive
        "3": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": enhanced_prompt,
                "clip": ["1", 1]
            }
        },

        # 4. Text encode negative
        "4": {
            "class_type": "CLIPTextEncode",
            "inputs": {
                "text": "low quality, blurry, static, slideshow, bad anatomy, deformed, ugly, distorted",
                "clip": ["1", 1]
            }
        },

        # 5. Empty latent for video frames
        "5": {
            "class_type": "EmptyLatentImage",
            "inputs": {
                "width": width,
                "height": height,
                "batch_size": frames
            }
        },

        # 6. KSampler - NOW USES ANIMATED MODEL FROM NODE 2!
        "6": {
            "class_type": "KSampler",
            "inputs": {
                "seed": int(time.time()),
                "steps": 28,  # Better quality
                "cfg": 7.5,  # Stronger prompts
                "sampler_name": "dpmpp_2m",  # Quality-optimized
                "scheduler": "karras",  # Smoother timesteps
                "positive": ["3", 0],
                "negative": ["4", 0],
                "latent_image": ["5", 0],
                "model": ["2", 0],  # Use evolved sampling with context window
                "denoise": 1.0
            }
        },

        # 7. VAE Decode
        "7": {
            "class_type": "VAEDecode",
            "inputs": {
                "samples": ["6", 0],
                "vae": ["1", 2]
            }
        },

        # 8. Save as video
        "8": {
            "class_type": "VHS_VideoCombine",
            "inputs": {
                "images": ["7", 0],
                "frame_rate": request.fps,
                "loop_count": 0,
                "filename_prefix": f"anime_video_{generation_id}",
                "format": "video/h264-mp4",
                "pingpong": False,
                "save_output": True
            }
        }
    }

    return workflow
@app.post("/api/generate")
async def generate_simple_video(request: Dict[str, Any], background_tasks: BackgroundTasks):
    """Simple generation endpoint for testing"""
    anime_request = AnimeGenerationRequest(
        prompt=request.get("prompt", "magical anime scene"),
        character=request.get("character", "anime character"),
        duration=request.get("duration", 5),
        frames=request.get("frames", 120),  # 3 seconds at 24fps
        use_apple_music=request.get("use_apple_music", False),
        track_id=request.get("track_id")
    )

    return await generate_professional_video(anime_request, background_tasks)

@app.get("/api/generations")
async def list_generations():
    """List all recent generations"""
    return {"generations": status_tracker.generations}

# Error monitoring endpoints@app.get("/api/errors/stats")async def get_error_stats():    """Get error statistics"""    return error_handler.get_stats()@app.get("/api/errors/recent")async def get_recent_errors(limit: int = 10):    """Get recent errors"""    try:        with open(error_handler.error_log) as f:            lines = f.readlines()            recent = [json.loads(line) for line in lines[-limit:]]            return {"errors": recent, "total": len(lines)}    except FileNotFoundError:        return {"errors": [], "total": 0}

# Serve Vue3 static files
app.mount("/assets", StaticFiles(directory="/opt/tower-anime-production/static/dist/assets"), name="assets")

@app.get("/")
async def root():
    return FileResponse("/opt/tower-anime-production/static/dist/index.html")


# Serve Vue3 static files
app.mount("/assets", StaticFiles(directory="/opt/tower-anime-production/static/dist/assets"), name="assets")

@app.get("/")
async def root():
    return FileResponse("/opt/tower-anime-production/static/dist/index.html")

if __name__ == "__main__":
    print("Starting Tower Anime Video Service on port 8328...")
    print(f"Output directory: {OUTPUT_DIR}")
    print("Using model: AOM3A1B.safetensors")
    uvicorn.run(app, host="127.0.0.1", port=8328)
