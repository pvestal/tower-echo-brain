#!/usr/bin/env python3
"""
Core Echo Brain query and conversation API routes
"""
import asyncio
import json
import time
import uuid
import logging
from datetime import datetime
from typing import Dict, List, Optional
from fastapi import APIRouter, HTTPException, BackgroundTasks

# Import models and services
from src.db.models import QueryRequest, QueryResponse
from src.db.database import database
from src.core.intelligence import intelligence_router
from src.services.conversation import conversation_manager

# Import external modules
from echo_brain_thoughts import echo_brain

# Import agentic persona for collaborative interaction
try:
    from src.core.agentic_persona import agentic_persona
except ImportError:
    agentic_persona = None

# Import memory components for conversation context
from src.memory.context_retrieval import ConversationContextRetriever
from src.memory.pronoun_resolver import PronounResolver
from src.memory.entity_extractor import EntityExtractor

logger = logging.getLogger(__name__)
router = APIRouter()

# Initialize memory components lazily
_context_retriever = None
_pronoun_resolver = None
_entity_extractor = None

def get_memory_components():
    """Get or create memory components"""
    global _context_retriever, _pronoun_resolver, _entity_extractor
    if _context_retriever is None:
        _context_retriever = ConversationContextRetriever()
        _pronoun_resolver = PronounResolver()
        _entity_extractor = EntityExtractor()
    return _context_retriever, _pronoun_resolver, _entity_extractor

@router.post("/api/echo/query", response_model=QueryResponse)
@router.post("/api/echo/chat", response_model=QueryResponse)
async def query_echo(request: QueryRequest):
    """Main query endpoint with intelligent routing and conversation management"""
    start_time = time.time()

    # Generate conversation ID if not provided
    if not request.conversation_id:
        request.conversation_id = str(uuid.uuid4())

    logger.info(f"üîç ECHO QUERY HANDLER - Query: {request.query[:50]}...")
    logger.info(f"üîç Request type: {getattr(request, 'request_type', 'NOT_SET')}")

    # Validate request_type field exists and log for debugging
    request_type = getattr(request, 'request_type', None)
    if not request_type:
        logger.warning(f"‚ö†Ô∏è request_type not set, defaulting to 'conversation'")
        request_type = 'conversation'
    else:
        logger.info(f"‚úÖ Request type validated: {request_type}")

    # CHECK REQUEST TYPE FIRST - Route to appropriate handler
    if request_type == 'system_command':
        logger.info("üö® ROUTING TO DIRECT SYSTEM COMMAND EXECUTION")

        # Direct execution with retry logic
        max_retries = 3
        for attempt in range(max_retries):
            try:
                logger.info(f"üîÑ System command attempt {attempt + 1}/{max_retries}: {request.query}")

                # Direct subprocess execution - no safety restrictions
                import subprocess

                # Execute the command directly using shell for full command support
                process = await asyncio.create_subprocess_shell(
                    request.query,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE,
                    cwd="/tmp"  # Execute in /tmp directory
                )

                stdout, stderr = await asyncio.wait_for(
                    process.communicate(),
                    timeout=30
                )

                stdout_text = stdout.decode('utf-8', errors='replace')
                stderr_text = stderr.decode('utf-8', errors='replace')

                processing_time = time.time() - start_time

                if process.returncode == 0:
                    logger.info(f"‚úÖ System command executed successfully on attempt {attempt + 1}")
                    output = stdout_text
                    if stderr_text:
                        output += f"\nSTDERR: {stderr_text}"
                    return QueryResponse(
                        response=output,
                        model_used="direct_executor",
                        intelligence_level="system_command",
                        processing_time=processing_time,
                        escalation_path=[f"system_command:{request.query[:20]}"],
                        conversation_id=request.conversation_id,
                        intent="system_command",
                        confidence=1.0,
                        requires_clarification=False,
                        clarifying_questions=[]
                    )
                else:
                    logger.warning(f"‚ö†Ô∏è System command failed on attempt {attempt + 1}: {stderr_text}")
                    if attempt == max_retries - 1:  # Last attempt
                        processing_time = time.time() - start_time
                        return QueryResponse(
                            response=f"Command failed after {max_retries} attempts.\nSTDOUT: {stdout_text}\nSTDERR: {stderr_text}\nExit code: {process.returncode}",
                            model_used="direct_executor",
                            intelligence_level="error",
                            processing_time=processing_time,
                            escalation_path=["system_command_failed"],
                            conversation_id=request.conversation_id,
                            intent="system_command",
                            confidence=0.0,
                            requires_clarification=False,
                            clarifying_questions=[]
                        )

            except asyncio.TimeoutError:
                logger.error(f"‚ùå System command timed out on attempt {attempt + 1}")
                if attempt == max_retries - 1:  # Last attempt
                    processing_time = time.time() - start_time
                    return QueryResponse(
                        response=f"Command timed out after {max_retries} attempts (30s timeout each)",
                        model_used="direct_executor",
                        intelligence_level="error",
                        processing_time=processing_time,
                        escalation_path=["system_command_timeout"],
                        conversation_id=request.conversation_id,
                        intent="system_command",
                        confidence=0.0,
                        requires_clarification=False,
                        clarifying_questions=[]
                    )
            except Exception as e:
                logger.error(f"‚ùå System command execution error on attempt {attempt + 1}: {e}")
                if attempt == max_retries - 1:  # Last attempt
                    processing_time = time.time() - start_time
                    return QueryResponse(
                        response=f"System command failed after {max_retries} attempts: {str(e)}",
                        model_used="direct_executor",
                        intelligence_level="error",
                        processing_time=processing_time,
                        escalation_path=["system_command_error"],
                        conversation_id=request.conversation_id,
                        intent="system_command",
                        confidence=0.0,
                        requires_clarification=False,
                        clarifying_questions=[]
                    )
                # Wait before retry
                await asyncio.sleep(0.5 * (attempt + 1))

    # CHECK FOR SLASH COMMANDS FIRST
    if request.query.strip().startswith('/'):
        try:
            from src.commands.command_handler import CommandHandler
            cmd_handler = CommandHandler()
            command, params = cmd_handler.parse_command(request.query)

            if command:
                logger.info(f"üîß Executing command: {command}")
                response_text = await cmd_handler.execute_command(command, params)

                processing_time = time.time() - start_time
                return QueryResponse(
                    response=response_text,
                    model_used="command_system",
                    intelligence_level="command",
                    processing_time=processing_time,
                    escalation_path=["direct_command"],
                    conversation_id=request.conversation_id,
                    intent="command",
                    confidence=1.0,
                    requires_clarification=False,
                    clarifying_questions=[]
                )
        except Exception as e:
            logger.error(f"Command execution failed: {e}")

    # Cognitive model selection if available
    selected_model = None
    complexity_score = None
    tier = None
    selection_reason = "default"
    try:
        from fixed_model_selector import ModelSelector
        selector = ModelSelector()
        selected_model, intelligence_level, selection_reason, complexity_score, tier = selector.select_model(
            request.query,
            request.intelligence_level if request.intelligence_level != "auto" else None
        )
        logger.info(f"üéØ Cognitive selection: {selected_model} ({intelligence_level}) - {selection_reason}")
        request.intelligence_level = intelligence_level
    except ImportError:
        logger.debug("Cognitive selector not available, using default routing")

    try:
        # Get memory components
        context_retriever, pronoun_resolver, entity_extractor = get_memory_components()

        # ============================================
        # STEP 1: Retrieve conversation history with memory system
        # ============================================
        conversation_context = await conversation_manager.get_conversation_context(request.conversation_id)
        logger.info(f"üîç CONVERSATION CONTEXT: {len(conversation_context.get('history', []))} messages retrieved")

        # Get enhanced history and entities from memory system
        memory_history = await context_retriever.get_recent_history(request.conversation_id)
        active_entities = await context_retriever.get_active_entities(request.conversation_id)
        logger.info(f"üß† MEMORY: {len(memory_history)} history messages, {len(active_entities)} entities")

        # ============================================
        # STEP 2: Extract entities from current query
        # ============================================
        current_entities = entity_extractor.extract(request.query)
        logger.info(f"üìã EXTRACTED ENTITIES: {current_entities}")

        # Merge with active entities from history
        all_entities = {**active_entities, **current_entities}
        logger.info(f"üîó MERGED ENTITIES: {all_entities}")

        # ============================================
        # STEP 3: Resolve pronouns using context
        # ============================================
        original_query = request.query
        resolved_query, resolved_entity = pronoun_resolver.resolve(
            original_query,
            all_entities  # Use merged entities for resolution
        )

        if resolved_entity:
            logger.info(f"‚ú® PRONOUN RESOLVED: '{original_query}' ‚Üí '{resolved_query}'")
            # Update the request query to use the resolved version
            request.query = resolved_query

        # Classify intent and extract parameters
        intent, confidence, intent_params = conversation_manager.classify_intent(
            request.query,
            conversation_context.get("history", [])
        )

        logger.info(f"üéØ Intent: {intent} (confidence: {confidence:.2f}) params: {intent_params}")

        # Check with agentic persona if we should ask questions
        should_ask_user = False
        contextual_question = None
        if agentic_persona:
            should_ask_user = agentic_persona.should_ask_question(
                request.query, intent, confidence
            )
            if should_ask_user:
                contextual_question = agentic_persona.get_contextual_question(
                    request.query, intent, conversation_context.get("history", [])
                )

        # Check if clarification is needed - BUT skip if pronoun was resolved
        needs_clarification = (should_ask_user or conversation_manager.needs_clarification(intent, confidence, request.query))
        if resolved_entity:
            # If we resolved a pronoun, we likely don't need clarification
            needs_clarification = False
            logger.info("üéØ Skipping clarification - pronoun was resolved")

        if needs_clarification:
            if contextual_question:
                clarifying_questions = [contextual_question]
                response_text = contextual_question
            else:
                clarifying_questions = conversation_manager.get_clarifying_questions(intent, request.query)
                response_text = "I'd like to better understand your request. Could you help clarify?"

            processing_time = time.time() - start_time

            response = QueryResponse(
                response=response_text,
                model_used="conversation_manager",
                intelligence_level="clarification",
                processing_time=processing_time,
                escalation_path=["clarification_needed"],
                requires_clarification=True,
                clarifying_questions=clarifying_questions,
                conversation_id=request.conversation_id,
                intent=intent,
                confidence=confidence,
                resolved_query=resolved_query if resolved_query != original_query else None,
                entities_extracted=active_entities if active_entities else None
            )

            conversation_manager.update_conversation(
                request.conversation_id, request.query, intent, response.response, True
            )

            # Log to database for learning WITH entities
            try:
                await save_conversation_with_entities(
                    database,
                    original_query,  # Use original query for database
                    response.response,
                    request.conversation_id,
                    all_entities,  # Include extracted and historical entities
                    response.model_used,
                    response.processing_time,
                    response.escalation_path,
                    request.user_id,
                    intent,
                    confidence,
                    True,  # requires_clarification
                    clarifying_questions,
                    complexity_score,
                    tier
                )
            except Exception as e:
                logger.error(f"‚ùå Clarification save_conversation FAILED: {e}")

            return response

        # Handle anime generation specifically
        if intent == "anime_generation":
            logger.info(f"üé¨ AUTO-ANIME: Generating anime with params: {intent_params}")
            try:
                import aiohttp
                prompt_text = intent_params.get('prompt', request.query)
                prompt_text = prompt_text.replace('generate anime', '').replace('create anime', '').strip()
                if not prompt_text:
                    prompt_text = "anime magical girl"

                async with aiohttp.ClientSession() as session:
                    payload = {"prompt": prompt_text}
                    async with session.post("http://localhost:8328/api/anime/generate",
                                           json=payload, timeout=60) as resp:
                        if resp.status == 200:
                            result = await resp.json()
                            job_id = result.get("job_id", "unknown")
                            response_text = (
                                f"‚úÖ Anime generation started!\n"
                                f"üé¨ Job ID: {job_id}\n"
                                f"üé≠ Prompt: {prompt_text}\n"
                                f"üìä Monitor: http://localhost:8328/api/anime/jobs/{job_id}"
                            )
                        else:
                            error_msg = await resp.text()
                            response_text = f"‚ùå Anime generation failed: {error_msg}"

                        response = QueryResponse(
                            response=response_text,
                            model_used="anime_service_8328",
                            intelligence_level="capability",
                            processing_time=time.time() - start_time,
                            escalation_path=["anime_generation_capability"],
                            conversation_id=request.conversation_id,
                            intent=intent,
                            confidence=confidence
                        )
            except Exception as e:
                logger.error(f"Anime generation error: {e}")
                response = QueryResponse(
                    response=f"‚ùå Anime generation error: {str(e)}",
                    model_used="anime_service_error",
                    intelligence_level="capability",
                    processing_time=time.time() - start_time,
                    escalation_path=["anime_error"],
                    conversation_id=request.conversation_id,
                    intent=intent,
                    confidence=confidence
                )

        # Handle other capability intents automatically
        elif intent in ['service_testing', 'service_debugging', 'service_monitoring', 'agent_delegation', 'inter_service_communication', 'image_generation', 'voice_generation', 'music_generation', 'code_review', 'code_refactor', 'code_modification']:
            response = await handle_capability_intent(intent, intent_params, request, request.conversation_id, start_time)

            conversation_manager.update_conversation(
                request.conversation_id, request.query, intent, response.response, False
            )

            try:
                # Save with entities
                await save_conversation_with_entities(
                    database,
                    original_query,  # Use original, not resolved query
                    response.response,
                    request.conversation_id,
                    all_entities,  # Include all extracted entities
                    response.model_used,
                    response.processing_time,
                    response.escalation_path,
                    request.user_id,
                    intent,
                    confidence,
                    False,
                    None,
                    complexity_score,
                    tier
                )
            except Exception as e:
                logger.error(f"‚ùå log_interaction FAILED: {e}")

            return response

        # Build context for model selection
        context = {
            "conversation_history": conversation_context.get("history", []),
            "user_id": request.user_id,
            "intent": intent,
            "intent_params": intent_params,
            "previous_failures": len([h for h in conversation_context.get("history", []) if "error" in h.get("response", "").lower()])
        }

        if request.intelligence_level and request.intelligence_level != "auto":
            context["requested_level"] = request.intelligence_level

        # FIXED: Always use direct query_model instead of broken progressive_escalation
        if selected_model:
            logger.info(f"üß† Using cognitively selected model: {selected_model} - {selection_reason}")
            result = await intelligence_router.query_model(selected_model, request.query, context)
            if result["success"]:
                result["intelligence_level"] = intelligence_level
                result["escalation_path"] = [f"cognitive_selection:{selected_model}"]
                result["decision_reason"] = selection_reason
            else:
                logger.warning(f"‚ö†Ô∏è Cognitive model {selected_model} failed, trying fallback model")
                result = await intelligence_router.query_model("llama3.2:3b", request.query, context)
        else:
            # Use direct query_model instead of broken progressive_escalation
            result = await intelligence_router.query_model("llama3.2:3b", request.query, context)

        if result["success"]:
            processing_time = time.time() - start_time

            response = QueryResponse(
                response=result["response"],
                model_used=result["model"],
                intelligence_level=result.get("intelligence_level", "standard"),
                processing_time=processing_time,
                escalation_path=result.get("escalation_path", []),
                conversation_id=request.conversation_id,
                intent=intent,
                confidence=confidence
            )

            conversation_manager.update_conversation(
                request.conversation_id, request.query, intent, response.response, False
            )

            try:
                # Save with entities
                await save_conversation_with_entities(
                    database,
                    original_query,  # Use original, not resolved query
                    response.response,
                    request.conversation_id,
                    all_entities,  # Include all extracted entities
                    response.model_used,
                    response.processing_time,
                    response.escalation_path,
                    request.user_id,
                    intent,
                    confidence,
                    False,
                    None,
                    complexity_score,
                    tier
                )
            except Exception as e:
                logger.error(f"‚ùå log_interaction FAILED: {e}")

            return response
        else:
            processing_time = time.time() - start_time
            error_message = f"‚ùå Query processing failed: {result.get('error', 'Unknown error')}"

            response = QueryResponse(
                response=error_message,
                model_used="error_handler",
                intelligence_level="error",
                processing_time=processing_time,
                escalation_path=["error"],
                conversation_id=request.conversation_id,
                intent=intent,
                confidence=0.0
            )

            return response

    except Exception as e:
        logger.error(f"Query processing failed: {e}")
        processing_time = time.time() - start_time

        response = QueryResponse(
            response=f"‚ùå Internal error: {str(e)}",
            model_used="error_handler",
            intelligence_level="error",
            processing_time=processing_time,
            escalation_path=["exception"],
            conversation_id=request.conversation_id,
            intent="error",
            confidence=0.0
        )

        return response

@router.get("/api/echo/brain")
async def get_brain_activity():
    """Get current brain activity and neural state"""
    try:
        brain_state = echo_brain.get_brain_state()
        return {
            "brain_activity": brain_state,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Failed to get brain activity: {e}")
        raise HTTPException(status_code=500, detail=f"Brain activity error: {str(e)}")

@router.get("/api/echo/thoughts/{thought_id}")
async def get_thought(thought_id: str):
    """Get specific thought by ID"""
    try:
        thought = echo_brain.get_thought(thought_id)
        if thought:
            return {"thought": thought, "timestamp": datetime.now().isoformat()}
        else:
            raise HTTPException(status_code=404, detail="Thought not found")
    except Exception as e:
        logger.error(f"Failed to get thought {thought_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Thought retrieval error: {str(e)}")

@router.get("/api/echo/conversation/{conversation_id}")
async def get_conversation(conversation_id: str):
    """Get conversation history"""
    try:
        conversation = conversation_manager.get_conversation_history(conversation_id)
        return {
            "conversation_id": conversation_id,
            "history": conversation,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Failed to get conversation {conversation_id}: {e}")
        raise HTTPException(status_code=500, detail=f"Conversation error: {str(e)}")

@router.get("/api/echo/conversations")
async def get_conversations():
    """Get all conversations"""
    try:
        conversations = conversation_manager.get_all_conversations()
        return {
            "conversations": conversations,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Failed to get conversations: {e}")
        raise HTTPException(status_code=500, detail=f"Conversations error: {str(e)}")

async def handle_capability_intent(intent: str, intent_params: dict, request: QueryRequest, conversation_id: str, start_time: float):
    """Handle capability-based intents"""
    from src.utils.helpers import safe_executor

    try:
        result = await safe_executor.execute_capability(intent, intent_params, request.query)
        processing_time = time.time() - start_time

        return QueryResponse(
            response=result["response"],
            model_used=result.get("model", "capability_handler"),
            intelligence_level="capability",
            processing_time=processing_time,
            escalation_path=[f"capability:{intent}"],
            conversation_id=conversation_id,
            intent=intent,
            confidence=1.0
        )
    except Exception as e:
        logger.error(f"Capability {intent} failed: {e}")
        processing_time = time.time() - start_time

        return QueryResponse(
            response=f"‚ùå Capability {intent} failed: {str(e)}",
            model_used="error_handler",
            intelligence_level="error",
            processing_time=processing_time,
            escalation_path=["capability_error"],
            conversation_id=conversation_id,
            intent=intent,
            confidence=0.0
        )