{
  "model_providers": {
    "primary": {
      "type": "ollama",
      "host": "http://localhost:11434",
      "gpu": "nvidia",
      "models": [
        "qwen2.5-coder:7b",
        "deepseek-r1:7b",
        "llama3.1:8b",
        "mistral:7b"
      ],
      "priority": 1
    },
    "secondary": {
      "type": "amd_gpu",
      "description": "AMD GPU for parallel workloads",
      "models": [
        "custom_models"
      ],
      "priority": 2
    },
    "fallback": {
      "type": "api_providers",
      "providers": [
        "claude",
        "openai",
        "deepseek"
      ],
      "priority": 3
    }
  },
  "default_provider": "ollama",
  "ollama_base_url": "http://localhost:11434"
}