"""
Fixed model selection for AI Assist
This module provides WORKING model selection
"""
import logging
from typing import Dict, Optional, Tuple
import time

logger = logging.getLogger(__name__)

class ModelSelector:
    """Proper model selection that actually works"""

    def __init__(self):
        # Models that EXIST and WORK
        self.model_hierarchy = {
            "quick": "tinyllama:latest",
            "standard": "llama3.2:3b",
            "professional": "mistral:7b-instruct",
            "expert": "qwen2.5-coder:32b",
            "genius": "llama3.1:70b",
            # Map legacy names
            "tiny": "tinyllama:latest",
            "small": "llama3.2:3b",
            "medium": "mistral:7b-instruct",
            "large": "qwen2.5-coder:32b",
            "xlarge": "llama3.1:70b"
        }

        # Specialized models for specific tasks
        self.specialized = {
            "code": "deepseek-coder-v2:16b",
            "vision": "llava:7b",
            "creative": "mixtral:8x7b"
        }

        # Performance expectations
        self.expected_times = {
            "tinyllama:latest": 1.0,
            "llama3.2:3b": 2.0,
            "mistral:7b-instruct": 4.0,
            "deepseek-coder-v2:16b": 5.0,
            "qwen2.5-coder:32b": 8.0,
            "llama3.1:70b": 15.0
        }

    def select_model(self, query: str, requested_level: Optional[str] = None) -> Tuple[str, str, str]:
        """
        Select optimal model for query
        Returns: (model_name, level, reasoning)
        """
        # If user explicitly requests a level, RESPECT IT
        if requested_level and requested_level != "auto":
            if requested_level in self.model_hierarchy:
                model = self.model_hierarchy[requested_level]
                logger.info(f"User requested {requested_level}: using {model}")
                return model, requested_level, f"User requested {requested_level}"
            else:
                logger.warning(f"Unknown level {requested_level}, using standard")
                return self.model_hierarchy["standard"], "standard", "Invalid level, using standard"

        # Auto-detect based on query analysis
        query_lower = query.lower()
        query_len = len(query)

        # Quick responses for simple queries
        if query_len < 25 and any(w in query_lower for w in ["hi", "hello", "hey", "thanks", "bye"]):
            return self.model_hierarchy["quick"], "quick", "Simple greeting/acknowledgment"

        # Code tasks need specialized model
        if any(w in query_lower for w in ["code", "function", "algorithm", "implement", "debug", "program"]):
            # Use specialized coder
            return self.specialized["code"], "expert", "Coding task - using specialized model"

        # Complex analytical tasks
        if query_len > 200 and any(w in query_lower for w in ["analyze", "architect", "design", "evaluate"]):
            return self.model_hierarchy["genius"], "genius", "Complex analytical task"

        # Technical questions
        if any(w in query_lower for w in ["explain", "how does", "what is", "why", "compare"]):
            if query_len > 100:
                return self.model_hierarchy["professional"], "professional", "Detailed technical question"
            else:
                return self.model_hierarchy["standard"], "standard", "Standard explanation needed"

        # Creative tasks
        if any(w in query_lower for w in ["story", "create", "imagine", "generate", "write"]):
            return self.specialized.get("creative", self.model_hierarchy["professional"]), "professional", "Creative task"

        # Default based on length
        if query_len < 50:
            return self.model_hierarchy["standard"], "standard", "Short general query"
        elif query_len < 150:
            return self.model_hierarchy["professional"], "professional", "Medium complexity query"
        else:
            return self.model_hierarchy["genius"], "genius", "Long complex query"

    def get_expected_time(self, model: str) -> float:
        """Get expected response time for model"""
        return self.expected_times.get(model, 5.0)
