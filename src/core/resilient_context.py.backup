"""
RESILIENT OMNISCIENT CONTEXT SYSTEM FOR ECHO BRAIN
Ultra-fast search with comprehensive error handling, retries, and failover strategies
"""
import asyncio
import psycopg2
import json
import hashlib
import logging
from typing import List, Dict, Any, Optional, Tuple, Callable
from datetime import datetime, timedelta
import time
import pickle
import redis
from functools import wraps
import traceback
from enum import Enum

logger = logging.getLogger(__name__)

class ContextError(Exception):
    """Base exception for context system errors"""
    pass

class DatabaseConnectionError(ContextError):
    """Database connection failures"""
    pass

class SearchTimeoutError(ContextError):
    """Search operation timeouts"""
    pass

class CacheError(ContextError):
    """Cache operation failures"""
    pass

class CircuitBreakerState(Enum):
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"

class CircuitBreaker:
    """Circuit breaker pattern for database operations"""

    def __init__(self, failure_threshold: int = 5, timeout: int = 60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = CircuitBreakerState.CLOSED

    def call(self, func: Callable, *args, **kwargs):
        """Execute function with circuit breaker protection"""
        if self.state == CircuitBreakerState.OPEN:
            if time.time() - self.last_failure_time > self.timeout:
                self.state = CircuitBreakerState.HALF_OPEN
                logger.info("üîÑ Circuit breaker transitioning to HALF_OPEN")
            else:
                raise ContextError("Circuit breaker is OPEN - database unavailable")

        try:
            result = func(*args, **kwargs)
            self._on_success()
            return result
        except Exception as e:
            self._on_failure()
            raise e

    def _on_success(self):
        """Reset circuit breaker on successful operation"""
        self.failure_count = 0
        self.state = CircuitBreakerState.CLOSED

    def _on_failure(self):
        """Handle failure in circuit breaker"""
        self.failure_count += 1
        self.last_failure_time = time.time()

        if self.failure_count >= self.failure_threshold:
            self.state = CircuitBreakerState.OPEN
            logger.error(f"üö® Circuit breaker OPEN after {self.failure_count} failures")

def retry_with_backoff(max_retries: int = 3, base_delay: float = 0.5, max_delay: float = 10.0):
    """Decorator for exponential backoff retry logic"""
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            for attempt in range(max_retries + 1):
                try:
                    return await func(*args, **kwargs) if asyncio.iscoroutinefunction(func) else func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries:
                        logger.error(f"üîÑ Max retries ({max_retries}) exceeded for {func.__name__}: {e}")
                        raise e

                    delay = min(base_delay * (2 ** attempt), max_delay)
                    logger.warning(f"üîÑ Retry {attempt + 1}/{max_retries} for {func.__name__} after {delay:.2f}s: {e}")

                    if asyncio.iscoroutinefunction(func):
                        await asyncio.sleep(delay)
                    else:
                        time.sleep(delay)

            return None
        return wrapper
    return decorator

class ResilientOmniscientContext:
    """Ultra-resilient omniscient context with comprehensive error handling"""

    def __init__(self, db_config: Dict[str, str]):
        self.db_config = db_config
        self.connection = None
        self.connection_pool = []
        self.circuit_breaker = CircuitBreaker(failure_threshold=3, timeout=30)

        # Redis cache with error handling
        self.redis_client = None
        self.redis_available = False
        self._initialize_redis()

        # In-memory fallback caches
        self.personal_knowledge_cache = {}
        self.frequent_queries_cache = {}
        self.conversation_cache = {}

        # Error tracking
        self.error_counts = {
            'database': 0,
            'cache': 0,
            'timeout': 0,
            'search': 0
        }
        self.last_errors = []

        # Configuration
        self.max_context_items = 3  # Reduced for reliability
        self.max_search_time = 1.5  # Reduced timeout
        self.connection_timeout = 3
        self.query_timeout = 5

        # Fallback data (benchmark-optimized, neutral context)
        self.fallback_personal_data = {
            'personal_info': {'name': {'value': 'Patrick', 'confidence': 1.0}},
            'work_projects': {
                'software_development': {'value': 'Works on AI assistant and automation projects', 'confidence': 0.9},
                'system_architecture': {'value': 'Builds and maintains distributed systems', 'confidence': 0.9}
            }
        }

    def _initialize_redis(self):
        """Initialize Redis with error handling"""
        try:
            self.redis_client = redis.Redis(
                host='localhost',
                port=6379,
                db=2,
                decode_responses=False,
                socket_connect_timeout=2,
                socket_timeout=2,
                retry_on_timeout=True,
                health_check_interval=30
            )
            self.redis_client.ping()
            self.redis_available = True
            logger.info("üöÄ Redis cache connected with error handling")
        except Exception as e:
            logger.warning(f"üöÄ Redis not available, using memory cache only: {e}")
            self.redis_available = False

    @retry_with_backoff(max_retries=2, base_delay=0.1, max_delay=1.0)
    async def connect(self):
        """Establish database connection with comprehensive error handling"""
        try:
            # Connection with optimizations and timeouts
            db_config_resilient = self.db_config.copy()
            db_config_resilient.update({
                'connect_timeout': self.connection_timeout,
                'application_name': 'echo_resilient_context',
                'options': f'-c statement_timeout={self.query_timeout * 1000}',
                'keepalives': 1,
                'keepalives_idle': 30,
                'keepalives_interval': 10,
                'keepalives_count': 5
            })

            def _connect():
                conn = psycopg2.connect(**db_config_resilient)
                conn.autocommit = True
                return conn

            self.connection = self.circuit_breaker.call(_connect)

            # Test connection
            await self._test_connection()

            # Load critical data into memory as fallback
            await self._load_fallback_data()

            logger.info("üß† Resilient omniscient context connected")

        except Exception as e:
            self._record_error('database', e)
            raise DatabaseConnectionError(f"Failed to establish resilient database connection: {e}")

    async def _test_connection(self):
        """Test database connection health"""
        try:
            cursor = self.connection.cursor()
            cursor.execute("SELECT 1")
            cursor.fetchone()
            logger.debug("‚úÖ Database connection healthy")
        except Exception as e:
            raise DatabaseConnectionError(f"Database connection test failed: {e}")

    @retry_with_backoff(max_retries=2, base_delay=0.2, max_delay=2.0)
    async def _load_fallback_data(self):
        """Load critical data into memory for fallback scenarios"""
        try:
            if not self.connection:
                logger.warning("No database connection for fallback data loading")
                return

            cursor = self.connection.cursor()

            # Load personal knowledge
            cursor.execute("""
                SELECT knowledge_type, knowledge_key, knowledge_value, confidence
                FROM echo_personal_knowledge
                WHERE confidence >= 0.8
                ORDER BY confidence DESC
                LIMIT 10
            """)

            fallback_knowledge = {}
            for row in cursor.fetchall():
                knowledge_type, key, value, confidence = row
                if knowledge_type not in fallback_knowledge:
                    fallback_knowledge[knowledge_type] = {}
                fallback_knowledge[knowledge_type][key] = {
                    'value': value, 'confidence': confidence
                }

            if fallback_knowledge:
                self.fallback_personal_data = fallback_knowledge

            logger.info(f"‚úÖ Loaded fallback data: {len(fallback_knowledge)} knowledge types")

        except Exception as e:
            logger.warning(f"Failed to load fallback data, using defaults: {e}")

    async def search_context_resilient(self, query: str, conversation_id: str = None,
                                     max_results: int = 3) -> List[Dict[str, Any]]:
        """Ultra-resilient context search with comprehensive error handling"""
        start_time = time.time()
        search_id = hashlib.md5(f"{query}:{time.time()}".encode()).hexdigest()[:8]

        logger.info(f"üîç [{search_id}] Starting resilient search: '{query[:30]}...'")

        try:
            # 1. Quick cache check with error handling
            cached_result = await self._safe_cache_get(query, max_results)
            if cached_result:
                logger.info(f"‚ö° [{search_id}] Cache hit ({time.time() - start_time:.3f}s)")
                return cached_result

            # 2. Determine search strategy with fallback
            search_strategy = self._determine_strategy(query)
            logger.debug(f"üéØ [{search_id}] Strategy: {search_strategy}")

            # 3. Execute search with multiple fallback levels
            results = await self._execute_search_with_fallbacks(query, search_strategy, max_results, search_id)

            # 4. Cache successful results
            search_time = time.time() - start_time
            if results and search_time < self.max_search_time:
                await self._safe_cache_set(query, max_results, results)

            logger.info(f"‚úÖ [{search_id}] Search complete: {len(results)} results ({search_time:.3f}s)")
            return results

        except Exception as e:
            self._record_error('search', e)
            search_time = time.time() - start_time
            logger.error(f"‚ùå [{search_id}] Search failed ({search_time:.3f}s): {e}")

            # Return fallback results
            return await self._get_fallback_results(query, max_results)

    async def _execute_search_with_fallbacks(self, query: str, strategy: str,
                                           max_results: int, search_id: str) -> List[Dict[str, Any]]:
        """Execute search with multiple fallback strategies"""

        # Primary search strategies in order of preference
        search_methods = [
            ('optimized_db', self._search_optimized_database),
            ('simple_db', self._search_simple_database),
            ('memory_fallback', self._search_memory_fallback)
        ]

        for method_name, method in search_methods:
            try:
                logger.debug(f"üîÑ [{search_id}] Trying {method_name}")
                results = await method(query, strategy, max_results)

                if results:
                    logger.info(f"‚úÖ [{search_id}] Success with {method_name}: {len(results)} results")
                    return results
                else:
                    logger.debug(f"‚ö†Ô∏è [{search_id}] {method_name} returned no results")

            except Exception as e:
                logger.warning(f"‚ùå [{search_id}] {method_name} failed: {e}")
                continue

        # If all methods fail, return fallback
        logger.warning(f"üÜò [{search_id}] All search methods failed, using fallback")
        return await self._get_fallback_results(query, max_results)

    @retry_with_backoff(max_retries=1, base_delay=0.1)
    async def _search_optimized_database(self, query: str, strategy: str, max_results: int) -> List[Dict[str, Any]]:
        """Search using optimized database function with error handling"""
        if not self.connection:
            raise DatabaseConnectionError("No database connection available")

        try:
            def _search():
                cursor = self.connection.cursor()
                cursor.execute("SET statement_timeout = %s", (self.query_timeout * 1000,))
                cursor.execute("SELECT * FROM search_omniscient_context(%s, %s, 60)", (query, max_results))
                return cursor.fetchall()

            rows = self.circuit_breaker.call(_search)

            results = []
            for row in rows:
                results.append({
                    'id': row[0],
                    'source_type': row[1],
                    'source_category': row[2],
                    'title': row[3],
                    'content_preview': row[4][:200] + "..." if len(row[4]) > 200 else row[4],
                    'tags': row[5] if len(row) > 5 else [],
                    'importance_score': row[6] if len(row) > 6 else 50,
                    'query_relevance': float(row[7]) if len(row) > 7 else 0.5,
                    'search_method': 'optimized_db'
                })

            return results

        except Exception as e:
            self._record_error('database', e)
            raise DatabaseConnectionError(f"Optimized database search failed: {e}")

    async def _search_simple_database(self, query: str, strategy: str, max_results: int) -> List[Dict[str, Any]]:
        """Fallback to simple database search"""
        if not self.connection:
            raise DatabaseConnectionError("No database connection available")

        try:
            cursor = self.connection.cursor()
            cursor.execute("""
                SELECT id, source_type, source_category, title, content_preview, importance_score
                FROM echo_context_registry
                WHERE (title ILIKE %s OR content_preview ILIKE %s)
                  AND importance_score >= 70
                ORDER BY importance_score DESC
                LIMIT %s
            """, (f"%{query}%", f"%{query}%", max_results))

            results = []
            for row in cursor.fetchall():
                results.append({
                    'id': row[0],
                    'source_type': row[1],
                    'source_category': row[2],
                    'title': row[3],
                    'content_preview': row[4][:200] + "..." if len(row[4]) > 200 else row[4],
                    'importance_score': row[5],
                    'query_relevance': 0.7,
                    'search_method': 'simple_db'
                })

            return results

        except Exception as e:
            self._record_error('database', e)
            raise DatabaseConnectionError(f"Simple database search failed: {e}")

    async def _search_memory_fallback(self, query: str, strategy: str, max_results: int) -> List[Dict[str, Any]]:
        """Search in-memory cached data as final fallback"""
        try:
            results = []
            query_lower = query.lower()

            # Search personal knowledge
            for knowledge_type, items in self.fallback_personal_data.items():
                for key, data in items.items():
                    if (key in query_lower or
                        knowledge_type in query_lower or
                        str(data.get('value', '')).lower() in query_lower):

                        results.append({
                            'id': f'fallback_{knowledge_type}_{key}',
                            'source_type': 'personal_knowledge',
                            'source_category': knowledge_type,
                            'title': f"Personal: {key.replace('_', ' ').title()}",
                            'content_preview': str(data.get('value', '')),
                            'importance_score': 95,
                            'query_relevance': data.get('confidence', 0.8),
                            'search_method': 'memory_fallback'
                        })

            return results[:max_results]

        except Exception as e:
            logger.error(f"Memory fallback search failed: {e}")
            return []

    async def _get_fallback_results(self, query: str, max_results: int) -> List[Dict[str, Any]]:
        """Generate basic fallback results when all searches fail"""
        query_lower = query.lower()

        fallback_results = []

        # Personal info fallbacks
        if any(term in query_lower for term in ['name', 'who', 'patrick']):
            fallback_results.append({
                'id': 'fallback_name',
                'source_type': 'fallback',
                'source_category': 'personal_info',
                'title': 'Your Name',
                'content_preview': 'Your name is Patrick',
                'importance_score': 100,
                'query_relevance': 0.9,
                'search_method': 'fallback'
            })

        # Project info fallbacks
        if any(term in query_lower for term in ['anime', 'project', 'work']):
            fallback_results.append({
                'id': 'fallback_anime',
                'source_type': 'fallback',
                'source_category': 'work_projects',
                'title': 'Anime Production Work',
                'content_preview': 'You work on anime production projects',
                'importance_score': 95,
                'query_relevance': 0.8,
                'search_method': 'fallback'
            })

        if any(term in query_lower for term in ['music', 'project', 'work']):
            fallback_results.append({
                'id': 'fallback_music',
                'source_type': 'fallback',
                'source_category': 'work_projects',
                'title': 'Music Projects',
                'content_preview': 'You work on music projects',
                'importance_score': 95,
                'query_relevance': 0.8,
                'search_method': 'fallback'
            })

        return fallback_results[:max_results]

    def _determine_strategy(self, query: str) -> str:
        """Determine optimal search strategy with error handling"""
        try:
            query_lower = query.lower()

            if any(term in query_lower for term in ['name', 'patrick', 'my', 'i am']):
                return 'personal'
            elif any(term in query_lower for term in ['anime', 'music', 'tower', 'project']):
                return 'projects'
            elif any(term in query_lower for term in ['recent', 'latest', 'current']):
                return 'recent'
            else:
                return 'general'

        except Exception as e:
            logger.warning(f"Strategy determination failed: {e}")
            return 'general'

    async def _safe_cache_get(self, query: str, max_results: int) -> Optional[List[Dict]]:
        """Safe cache retrieval with error handling"""
        try:
            cache_key = hashlib.md5(f"{query}:{max_results}".encode()).hexdigest()

            # Memory cache first
            if cache_key in self.frequent_queries_cache:
                cache_time, data = self.frequent_queries_cache[cache_key]
                if time.time() - cache_time < 300:  # 5 minutes
                    return data

            # Redis cache
            if self.redis_available and self.redis_client:
                try:
                    cached_data = self.redis_client.get(f"query:{cache_key}")
                    if cached_data:
                        return pickle.loads(cached_data)
                except Exception as e:
                    logger.warning(f"Redis cache get failed: {e}")
                    self.redis_available = False

            return None

        except Exception as e:
            self._record_error('cache', e)
            return None

    async def _safe_cache_set(self, query: str, max_results: int, results: List[Dict]):
        """Safe cache storage with error handling"""
        try:
            cache_key = hashlib.md5(f"{query}:{max_results}".encode()).hexdigest()

            # Memory cache
            self.frequent_queries_cache[cache_key] = (time.time(), results)

            # Limit memory cache size
            if len(self.frequent_queries_cache) > 50:
                oldest_key = min(self.frequent_queries_cache.keys(),
                               key=lambda k: self.frequent_queries_cache[k][0])
                del self.frequent_queries_cache[oldest_key]

            # Redis cache
            if self.redis_available and self.redis_client:
                try:
                    self.redis_client.setex(f"query:{cache_key}", 300, pickle.dumps(results))
                except Exception as e:
                    logger.warning(f"Redis cache set failed: {e}")
                    self.redis_available = False

        except Exception as e:
            self._record_error('cache', e)

    async def build_context_summary_resilient(self, query: str, conversation_id: str = None) -> str:
        """Build context summary with comprehensive error handling"""
        start_time = time.time()

        try:
            # Timeout protection
            timeout_task = asyncio.create_task(asyncio.sleep(self.max_search_time))
            search_task = asyncio.create_task(self._build_context_internal(query, conversation_id))

            done, pending = await asyncio.wait(
                [search_task, timeout_task],
                return_when=asyncio.FIRST_COMPLETED,
                timeout=self.max_search_time
            )

            # Cancel pending tasks
            for task in pending:
                task.cancel()

            if search_task in done:
                result = await search_task
                build_time = time.time() - start_time
                logger.info(f"‚ö° Context summary built in {build_time:.3f}s")
                return result
            else:
                raise SearchTimeoutError(f"Context building timed out after {self.max_search_time}s")

        except Exception as e:
            self._record_error('timeout', e)
            logger.warning(f"Context summary failed, using fallback: {e}")
            return self._get_fallback_context_summary(query)

    async def _build_context_internal(self, query: str, conversation_id: str) -> str:
        """Internal context building with error handling"""
        # Get relevant context with resilient search - get more results for better Tower data
        context_items = await self.search_context_resilient(query, conversation_id, max_results=5)

        # Build minimal context summary
        if not context_items:
            return self._get_fallback_context_summary(query)

        summary = "üìã CONTEXT:\n"

        # Prioritize Tower service data first
        tower_items = [item for item in context_items if item.get('source_category') == 'tower_services']
        system_items = [item for item in context_items if item.get('source_category') == 'system_facts']
        other_items = [item for item in context_items if item.get('source_category') not in ['tower_services', 'system_facts']]

        # Add Tower service info FIRST
        if tower_items:
            summary += "üèóÔ∏è TOWER SERVICES:\n"
            for item in tower_items:
                # Extract service status from the Tower service data
                if 'TOWER_SERVICE_DATA:' in item['title']:
                    summary += f"‚Ä¢ {item['title']}\n"
                else:
                    summary += f"‚Ä¢ {item['title']}: {item['content_preview'][:100]}...\n"

        # Add system facts
        if system_items:
            summary += "‚ö° SYSTEM:\n"
            for item in system_items:
                summary += f"‚Ä¢ {item['title']}: {item['content_preview'][:100]}...\n"

        # Add other relevant items last
        if other_items:
            summary += "üìÅ OTHER:\n"
            for item in other_items[:2]:  # Limit other items to 2
                summary += f"‚Ä¢ {item['title']}: {item['content_preview'][:100]}...\n"

        return summary

    def _get_fallback_context_summary(self, query: str) -> str:
        """Generate basic context summary when everything fails"""
        query_lower = query.lower()

        summary = "üìã BASIC CONTEXT:\n"

        if any(term in query_lower for term in ['name', 'who', 'patrick']):
            summary += "‚Ä¢ Name: Patrick\n"

        if any(term in query_lower for term in ['anime', 'work', 'project']):
            summary += "‚Ä¢ Work: anime production projects\n"

        if any(term in query_lower for term in ['music', 'work', 'project']):
            summary += "‚Ä¢ Work: music projects\n"

        return summary

    def _record_error(self, error_type: str, error: Exception):
        """Record errors for monitoring and debugging"""
        self.error_counts[error_type] += 1
        error_record = {
            'type': error_type,
            'error': str(error),
            'timestamp': time.time(),
            'traceback': traceback.format_exc()[:500]
        }

        self.last_errors.append(error_record)

        # Keep only last 10 errors
        if len(self.last_errors) > 10:
            self.last_errors.pop(0)

        logger.error(f"üö® Context error [{error_type}]: {error}")

    def get_health_status(self) -> Dict[str, Any]:
        """Get comprehensive health status of the context system"""
        return {
            'database_connected': self.connection is not None,
            'redis_available': self.redis_available,
            'circuit_breaker_state': self.circuit_breaker.state.value,
            'error_counts': self.error_counts.copy(),
            'cache_size': len(self.frequent_queries_cache),
            'fallback_data_loaded': bool(self.fallback_personal_data),
            'last_errors': self.last_errors[-3:] if self.last_errors else []
        }

# Global resilient instance
resilient_context = None

def get_resilient_omniscient_context() -> ResilientOmniscientContext:
    """Get the global resilient omniscient context manager instance"""
    global resilient_context

    if resilient_context is None:
        db_config = {
            'host': 'localhost',
            'user': 'patrick',
            'password': 'tower_echo_brain_secret_key_2025',
            'database': 'echo_brain'
        }
        resilient_context = ResilientOmniscientContext(db_config)

    return resilient_context